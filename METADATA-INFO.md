# –ù–∞—Ç–∏–≤–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ LightRAG —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π API endpoint

## –ü—Ä–æ–±–ª–µ–º–∞

LightRAG –∏–º–µ–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ Python API (`insert_custom_kg`, `file_paths`, `source_id`), –Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ REST API endpoints (`/documents/upload`, `/documents/text`) **–Ω–µ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ**.

## –†–µ—à–µ–Ω–∏–µ: –í–∞—Ä–∏–∞–Ω—Ç –ë - –ù–æ–≤—ã–π endpoint `/documents/upload_with_metadata`

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

```
–§–∞–π–ª + –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ ‚Üí –ù–æ–≤—ã–π endpoint ‚Üí –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ ‚Üí Chunking (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π) ‚Üí insert_custom_kg ‚Üí LightRAG
                                                                                      ‚Üì
                                                                          –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞ –∫–∞–∂–¥–æ–º —á–∞–Ω–∫–µ
```

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ–¥—Ö–æ–¥–∞

1. ‚úÖ **–ù–∞—Ç–∏–≤–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö** - –ª—é–±—ã–µ –ø–æ–ª—è, –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
2. ‚úÖ **REST API** - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Node-RED, Flowise, HTTP –∫–ª–∏–µ–Ω—Ç–∞–º–∏
3. ‚úÖ **–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π chunking** - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π `ChunkedTextSplitter` –∏–∑ LightRAG
4. ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ entities** - LLM –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–±—ã—á–Ω–æ
5. ‚úÖ **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤** - PDF, DOCX, TXT, CSV, JSON (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã)
6. ‚úÖ **–ù–µ —Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö endpoints** - –æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
7. ‚úÖ **–ß–∏—Å—Ç–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö** - –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ –≥—Ä–∞—Ñ–µ –∑–Ω–∞–Ω–∏–π

### –û—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–∏—Å–∞

| –ö—Ä–∏—Ç–µ—Ä–∏–π | –ü—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–∏—Å | –ù–æ–≤—ã–π endpoint |
|----------|---------------|----------------|
| **–•—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö** | –í—Å—Ç—Ä–æ–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç | –û—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ –ë–î |
| **–ó–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞** | ‚ö†Ô∏è –î–∞ | ‚úÖ –ù–µ—Ç |
| **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º** | ‚ùå –°–ª–æ–∂–Ω–æ | ‚úÖ –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø |
| **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã** | ‚ö†Ô∏è –ù—É–∂–µ–Ω –ø—Ä–æ–∫—Å–∏ | ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–æ –≤ LightRAG |
| **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö** | ‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ | ‚úÖ –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –ø–æ–ª—è–º |
| **–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞** | 1-2 –¥–Ω—è | 0.5-1 –¥–µ–Ω—å |

---

## –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è

### 1. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤

**‚úÖ –î–∞, —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã LightRAG:**

LightRAG API —É–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:

```python
SUPPORTED_EXTENSIONS = {
    '.txt', '.md',           # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    '.pdf',                  # PDF (PyPDF2, pdfplumber)
    '.docx', '.doc',         # Word (python-docx)
    '.pptx',                 # PowerPoint
    '.csv',                  # CSV (pandas)
    '.json', '.xml'          # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
}
```

**–ù–æ–≤—ã–π endpoint –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –ø–∞—Ä—Å–µ—Ä—ã:**

```python
@router.post("/documents/upload_with_metadata")
async def upload_with_metadata(
    file: UploadFile = File(...),
    metadata: str = Form(default="{}")
):
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in SUPPORTED_EXTENSIONS:
        raise HTTPException(415, f"Unsupported file type: {file_ext}")

    # 2. –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ (–í–°–¢–†–û–ï–ù–ù–´–ï —Ñ—É–Ω–∫—Ü–∏–∏ LightRAG)
    file_content = await file.read()

    if file_ext == '.pdf':
        text = parse_pdf(file_content)      # –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    elif file_ext == '.docx':
        text = parse_docx(file_content)     # –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    elif file_ext in ['.txt', '.md']:
        text = file_content.decode('utf-8')
    elif file_ext == '.csv':
        text = parse_csv(file_content)      # –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    # ... –∏ —Ç.–¥.

    # 3. –î–∞–ª—å–Ω–µ–π—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    ...
```

---

### 2. –ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–Ω–µ —Ç–æ–ª—å–∫–æ source_id)

**‚úÖ –õ–Æ–ë–´–ï –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - –Ω–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π!**

LightRAG –Ω–µ –∏–º–µ–µ—Ç –∂–µ—Å—Ç–∫–æ–π —Å—Ö–µ–º—ã –¥–ª—è –ø–æ–ª–µ–π. –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –∫–∞–∫ JSON/Dict.

#### –ü—Ä–∏–º–µ—Ä: –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö

```python
custom_kg = {
    "chunks": [{
        "content": "—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–ª—è LightRAG
        "source_id": "doc-123",              # ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        "file_path": "/reports/2025/q1.pdf", # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        # –ü–†–û–ò–ó–í–û–õ–¨–ù–´–ï –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - —á—Ç–æ —É–≥–æ–¥–Ω–æ!
        "author": "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á",
        "department": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª",
        "date_created": "2025-01-15",
        "date_modified": "2025-01-20",
        "version": "1.2",
        "category": "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
        "subcategory": "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç",
        "tags": ["Q1", "2025", "–≤–∞–∂–Ω—ã–π", "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ"],
        "confidentiality": "internal",
        "project_id": "PROJ-2025-001",
        "approver": "–ü–µ—Ç—Ä–æ–≤ –ü.–ü.",
        "status": "approved",
        "language": "ru",
        "pages": 15,
        "custom_field_1": "–ª—é–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
        "custom_field_2": {"nested": "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", "data": [1, 2, 3]},
        "metadata_version": "2.0"
        # ... –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ –ø–æ–ª—è!
    }]
}
```

#### –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–Ω—É—Ç—Ä–∏ LightRAG

```python
# LightRAG —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –µ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
chunk_data = {
    "content": "...",
    "source_id": "...",
    **metadata  # ‚Üê –í—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
}

# –í KV Storage (JSON/Redis/PostgreSQL)
await kv_storage.set(chunk_id, chunk_data)

# –ü—Ä–∏ —á—Ç–µ–Ω–∏–∏ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ–ª—è –æ–±—Ä–∞—Ç–Ω–æ
retrieved_chunk = await kv_storage.get(chunk_id)
print(retrieved_chunk["author"])        # "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á"
print(retrieved_chunk["department"])    # "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª"
```

---

### 3. –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º chunking –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è entities

**‚úÖ –î–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π pipeline LightRAG**

#### –í–∞—Ä–∏–∞–Ω—Ç –ê: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π chunking –ë–ï–ó –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–∞ —á–∞–Ω–∫–∞—Ö (–ø—Ä–æ—Å—Ç–æ–π)

```python
@router.post("/documents/upload_simple")
async def upload_simple(
    file: UploadFile = File(...),
    metadata: str = Form(default="{}")
):
    # –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
    text = await parse_file(file)
    meta = json.loads(metadata)

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –í –ù–ê–ß–ê–õ–û —Ç–µ–∫—Å—Ç–∞ (–∫–∞–∫ –±–ª–æ–∫)
    enriched_text = f"""
[DOCUMENT_METADATA]
{json.dumps(meta, ensure_ascii=False, indent=2)}
[/DOCUMENT_METADATA]

{text}
"""

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π insert() - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π chunking
    await rag.insert(
        enriched_text,
        file_paths=[file.filename],
        ids=[meta.get("source_id")]
    )
```

**–ú–∏–Ω—É—Å:** –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤–æ–º —á–∞–Ω–∫–µ, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ –∏—Ö –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç.

---

#### –í–∞—Ä–∏–∞–Ω—Ç –ë: Manual chunking + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞ –ö–ê–ñ–î–û–ú —á–∞–Ω–∫–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```python
from lightrag.core.text_splitter import ChunkedTextSplitter
import uuid

@router.post("/documents/upload_with_metadata")
async def upload_with_metadata(
    file: UploadFile = File(...),
    metadata: str = Form(default="{}"),
    chunk_size: int = Form(default=1200),
    chunk_overlap: int = Form(default=200)
):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

    Args:
        file: –§–∞–π–ª (PDF, DOCX, TXT, CSV, JSON, etc.)
        metadata: JSON —Å—Ç—Ä–æ–∫–∞ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1200)
        chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ –≤ —Ç–æ–∫–µ–Ω–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 200)

    Returns:
        {
            "track_id": "...",
            "chunks_created": 5,
            "metadata_applied": true
        }
    """

    # 1. –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ–º –í–°–¢–†–û–ï–ù–ù–´–ï –ø–∞—Ä—Å–µ—Ä—ã)
    text = await parse_file(file)

    # 2. –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    meta = json.loads(metadata)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º source_id –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
    source_id = meta.get("source_id", str(uuid.uuid4()))
    file_path = meta.get("file_path", file.filename)

    # 3. –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–¢–†–û–ï–ù–ù–´–ô chunker –∏–∑ LightRAG
    splitter = ChunkedTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        tokenizer=rag.tokenizer  # –ò—Å–ø–æ–ª—å–∑—É–µ–º tokenizer –∏–∑ LightRAG instance
    )

    chunks = splitter.split_text(text)

    # 4. –°–æ–∑–¥–∞–µ–º custom_kg —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ö–ê–ñ–î–û–ì–û —á–∞–Ω–∫–∞
    custom_kg = {
        "chunks": [
            {
                "content": chunk,
                "source_id": source_id,
                "file_path": file_path,
                "chunk_index": i,              # –ù–æ–º–µ—Ä —á–∞–Ω–∫–∞
                "total_chunks": len(chunks),   # –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ

                # –í–°–ï –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
                **{k: v for k, v in meta.items()
                   if k not in ["source_id", "file_path"]}
            }
            for i, chunk in enumerate(chunks)
        ]
    }

    # 5. –í—Å—Ç–∞–≤–ª—è–µ–º - LightRAG –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
    #    - –ò–∑–≤–ª–µ—á–µ—Ç entities —á–µ—Ä–µ–∑ LLM
    #    - –°–æ–∑–¥–∞—Å—Ç relationships —á–µ—Ä–µ–∑ LLM
    #    - –°–æ—Ö—Ä–∞–Ω–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞ –∫–∞–∂–¥–æ–º —á–∞–Ω–∫–µ
    track_id = await rag.insert_custom_kg(custom_kg)

    return {
        "track_id": track_id,
        "chunks_created": len(chunks),
        "metadata_applied": True,
        "source_id": source_id
    }
```

**‚úÖ –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ LightRAG –ø–æ—Å–ª–µ –≤—ã–∑–æ–≤–∞ `insert_custom_kg`:**

1. **Chunking —É–∂–µ —Å–¥–µ–ª–∞–Ω** - –ø–µ—Ä–µ–¥–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏
2. **LLM –∏–∑–≤–ª–µ–∫–∞–µ—Ç entities** –∏–∑ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
3. **LLM —Å–æ–∑–¥–∞–µ—Ç relationships** –º–µ–∂–¥—É entities –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
4. **–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤** - embedding –º–æ–¥–µ–ª—å
5. **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ storage** - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ —Å –∫–∞–∂–¥—ã–º —á–∞–Ω–∫–æ–º

**–†–µ–∑—É–ª—å—Ç–∞—Ç –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö:**

```json
// Chunk 1
{
  "chunk_id": "chunk-1",
  "content": "–í –ø–µ—Ä–≤–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ 2025 –≥–æ–¥–∞...",
  "source_id": "report-q1-2025",
  "file_path": "/reports/q1-2025.pdf",
  "chunk_index": 0,
  "total_chunks": 5,
  "author": "–ò–≤–∞–Ω–æ–≤",
  "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
  "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
  "date_created": "2025-01-15"
}

// Chunk 2
{
  "chunk_id": "chunk-2",
  "content": "–í—ã—Ä—É—á–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–æ—Å—Ç–∞–≤–∏–ª–∞...",
  "source_id": "report-q1-2025",  // –¢–æ—Ç –∂–µ source_id
  "file_path": "/reports/q1-2025.pdf",
  "chunk_index": 1,
  "total_chunks": 5,
  "author": "–ò–≤–∞–Ω–æ–≤",              // –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞ –∫–∞–∂–¥–æ–º —á–∞–Ω–∫–µ!
  "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
  "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
  "date_created": "2025-01-15"
}
```

---

### 4. –†–∞–±–æ—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ

#### 4.1 –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ source_id (–≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞)

**‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –∫–æ—Ä–æ–±–∫–∏:**

```python
# Python API
result = await rag.query(
    "–ö–∞–∫–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏?",
    param=QueryParam(
        mode="hybrid",
        ids=["report-q1-2025", "report-q2-2025"]  # –§–∏–ª—å—Ç—Ä –ø–æ source_id
    )
)
```

**REST API:**

```bash
curl -X POST "http://localhost:9621/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏",
    "mode": "hybrid",
    "ids": ["report-q1-2025", "report-q2-2025"]
  }'
```

---

#### 4.2 –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º (—Ç—Ä–µ–±—É–µ—Ç –Ω–æ–≤—ã–π endpoint)

**–ù–æ–≤—ã–π endpoint: `/query_with_filter`**

```python
@router.post("/query_with_filter")
async def query_with_filter(
    query: str,
    metadata_filter: dict = {},  # –§–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
    mode: str = "hybrid",
    include_metadata: bool = True
):
    """
    –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        metadata_filter: –§–∏–ª—å—Ç—Ä—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä:
            {
                "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
                "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
                "date_created": "2025-01-15"
            }
        mode: –†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ (local/global/hybrid/mix)
        include_metadata: –í–∫–ª—é—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–≤–µ—Ç

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    """

    # 1. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ KV Storage
    all_chunks = await rag.kv_storage.get_all_chunks()

    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
    filtered_source_ids = set()

    for chunk_id, chunk_data in all_chunks.items():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –í–°–ï–ú —Ñ–∏–ª—å—Ç—Ä–∞–º
        match = all(
            chunk_data.get(key) == value
            for key, value in metadata_filter.items()
        )

        if match:
            filtered_source_ids.add(chunk_data.get("source_id"))

    # 3. –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if not filtered_source_ids:
        return {
            "answer": "–î–æ–∫—É–º–µ–Ω—Ç—ã —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.",
            "filtered_documents": 0,
            "sources": []
        }

    # 4. –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ –ø–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    result = await rag.query(
        query,
        param=QueryParam(
            mode=mode,
            ids=list(filtered_source_ids)  # –¢–æ–ª—å–∫–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ
        )
    )

    # 5. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–≤–µ—Ç
    if include_metadata:
        result.chunks_with_metadata = await _enrich_with_metadata(
            result.source_chunks
        )

    result.filtered_documents = len(filtered_source_ids)

    return result


async def _enrich_with_metadata(chunk_ids: list[str]) -> list[dict]:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —á–∞–Ω–∫–∞–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö"""
    chunks_with_meta = []

    for chunk_id in chunk_ids:
        chunk_data = await rag.kv_storage.get(chunk_id)

        chunks_with_meta.append({
            "content": chunk_data["content"],
            "metadata": {
                k: v for k, v in chunk_data.items()
                if k not in ["content", "embedding"]  # –í—Å–µ –∫—Ä–æ–º–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –≤–µ–∫—Ç–æ—Ä–æ–≤
            }
        })

    return chunks_with_meta
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**

```bash
# –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
curl -X POST "http://localhost:9621/query_with_filter" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤—ã—Ä—É—á–∫–∏",
    "metadata_filter": {
      "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
      "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
      "author": "–ò–≤–∞–Ω–æ–≤"
    },
    "mode": "hybrid",
    "include_metadata": true
  }'
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**

```json
{
  "answer": "–ö–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤—ã—Ä—É—á–∫–∏ –∑–∞ Q1 2025 —Å–æ—Å—Ç–∞–≤–∏–ª–∏ 150 –º–ª–Ω —Ä—É–±...",
  "filtered_documents": 3,
  "chunks_with_metadata": [
    {
      "content": "–í –ø–µ—Ä–≤–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ 2025 –≥–æ–¥–∞ –≤—ã—Ä—É—á–∫–∞ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 150 –º–ª–Ω —Ä—É–±...",
      "metadata": {
        "source_id": "report-q1-2025",
        "file_path": "/reports/q1-2025.pdf",
        "chunk_index": 0,
        "total_chunks": 5,
        "author": "–ò–≤–∞–Ω–æ–≤",
        "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
        "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
        "date_created": "2025-01-15",
        "tags": ["Q1", "2025", "–≤–∞–∂–Ω—ã–π"]
      }
    },
    {
      "content": "–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è –ø—Ä–∏–±—ã–ª—å —É–≤–µ–ª–∏—á–∏–ª–∞—Å—å –Ω–∞ 20%...",
      "metadata": {
        "source_id": "report-q1-2025",
        "file_path": "/reports/q1-2025.pdf",
        "chunk_index": 2,
        "total_chunks": 5,
        "author": "–ò–≤–∞–Ω–æ–≤",
        "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
        "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
        "date_created": "2025-01-15",
        "tags": ["Q1", "2025", "–≤–∞–∂–Ω—ã–π"]
      }
    }
  ],
  "sources": [
    {
      "source_id": "report-q1-2025",
      "file_path": "/reports/q1-2025.pdf",
      "relevance_score": 0.95
    }
  ]
}
```

---

#### 4.3 –°–ª–æ–∂–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã (–¥–∏–∞–ø–∞–∑–æ–Ω—ã, —Å–ø–∏—Å–∫–∏, —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è)

**–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è:**

```python
@router.post("/query_advanced_filter")
async def query_advanced_filter(
    query: str,
    filters: dict = {}
):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤

    –ü—Ä–∏–º–µ—Ä—ã —Ñ–∏–ª—å—Ç—Ä–æ–≤:
    {
        "department": {"$eq": "—Ñ–∏–Ω–∞–Ω—Å—ã"},           # –†–∞–≤–Ω–æ
        "date_created": {"$gte": "2025-01-01"},     # –ë–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ
        "tags": {"$contains": "–≤–∞–∂–Ω—ã–π"},            # –°–æ–¥–µ—Ä–∂–∏—Ç —ç–ª–µ–º–µ–Ω—Ç
        "author": {"$in": ["–ò–≤–∞–Ω–æ–≤", "–ü–µ—Ç—Ä–æ–≤"]},    # –í —Å–ø–∏—Å–∫–µ
        "category": {"$regex": "–æ—Ç—á–µ—Ç.*"}           # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
    }
    """

    all_chunks = await rag.kv_storage.get_all_chunks()
    filtered_source_ids = set()

    for chunk_id, chunk_data in all_chunks.items():
        match = True

        for field, condition in filters.items():
            if isinstance(condition, dict):
                # –û–ø–µ—Ä–∞—Ç–æ—Ä—ã
                if "$eq" in condition:
                    match &= chunk_data.get(field) == condition["$eq"]
                elif "$gte" in condition:
                    match &= chunk_data.get(field, "") >= condition["$gte"]
                elif "$lte" in condition:
                    match &= chunk_data.get(field, "") <= condition["$lte"]
                elif "$contains" in condition:
                    match &= condition["$contains"] in chunk_data.get(field, [])
                elif "$in" in condition:
                    match &= chunk_data.get(field) in condition["$in"]
                elif "$regex" in condition:
                    import re
                    pattern = re.compile(condition["$regex"])
                    match &= bool(pattern.match(str(chunk_data.get(field, ""))))
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ
                match &= chunk_data.get(field) == condition

        if match:
            filtered_source_ids.add(chunk_data.get("source_id"))

    # –ü–æ–∏—Å–∫ –ø–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    result = await rag.query(
        query,
        param=QueryParam(ids=list(filtered_source_ids))
    )

    return result
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**

```bash
curl -X POST "http://localhost:9621/query_advanced_filter" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏",
    "filters": {
      "department": {"$eq": "—Ñ–∏–Ω–∞–Ω—Å—ã"},
      "date_created": {"$gte": "2025-01-01"},
      "tags": {"$contains": "–≤–∞–∂–Ω—ã–π"},
      "author": {"$in": ["–ò–≤–∞–Ω–æ–≤", "–ü–µ—Ç—Ä–æ–≤"]}
    }
  }'
```

---

## –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
lightrag/api/
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ documents.py              # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
‚îÇ   ‚îî‚îÄ‚îÄ documents_metadata.py     # –ù–û–í–´–ô —Ñ–∞–π–ª
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ file_parsers.py          # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
‚îî‚îÄ‚îÄ main.py                       # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Ä–æ—É—Ç–æ–≤
```

### –ö–æ–¥: `lightrag/api/routes/documents_metadata.py`

```python
"""
–ù–æ–≤—ã–µ endpoints –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from pathlib import Path
import json
import uuid
from datetime import datetime

from lightrag.core.text_splitter import ChunkedTextSplitter
from lightrag.core.base import QueryParam
from ..utils.file_parsers import parse_file

router = APIRouter(prefix="/documents", tags=["documents-metadata"])

# –ü–æ–ª—É—á–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π LightRAG instance
from ..main import get_rag_instance
rag = get_rag_instance()


class MetadataFilter(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º"""
    field: str
    operator: str = "eq"  # eq, gte, lte, contains, in, regex
    value: Any


class QueryWithFilterRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º"""
    query: str
    metadata_filter: Optional[Dict[str, Any]] = {}
    mode: str = "hybrid"
    include_metadata: bool = True
    top_k: int = 60


@router.post("/upload_with_metadata")
async def upload_with_metadata(
    file: UploadFile = File(...),
    metadata: str = Form(default="{}"),
    chunk_size: int = Form(default=1200),
    chunk_overlap: int = Form(default=200)
):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: PDF, DOCX, TXT, MD, CSV, JSON, XML

    Args:
        file: –§–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        metadata: JSON —Å—Ç—Ä–æ–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä:
            {
                "source_id": "doc-123",        // –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
                "file_path": "/path/to/file",  // –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
                "author": "–ò–≤–∞–Ω–æ–≤ –ò.–ò.",
                "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
                "date_created": "2025-01-15",
                "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
                "tags": ["Q1", "–≤–∞–∂–Ω—ã–π"],
                ... –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ –ø–æ–ª—è
            }
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
        chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ –≤ —Ç–æ–∫–µ–Ω–∞—Ö

    Returns:
        {
            "track_id": "...",
            "source_id": "...",
            "chunks_created": 5,
            "metadata_applied": true
        }
    """

    try:
        # 1. –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        try:
            meta = json.loads(metadata)
        except json.JSONDecodeError:
            raise HTTPException(400, "Invalid JSON in metadata field")

        # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º source_id –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        source_id = meta.get("source_id", f"doc-{uuid.uuid4()}")
        file_path = meta.get("file_path", file.filename)

        # 3. –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã)
        text = await parse_file(file)

        # 4. Chunking (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π splitter)
        splitter = ChunkedTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            tokenizer=rag.tokenizer
        )

        chunks = splitter.split_text(text)

        # 5. –°–æ–∑–¥–∞–µ–º custom_kg —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —á–∞–Ω–∫–µ
        custom_kg = {
            "chunks": [
                {
                    "content": chunk,
                    "source_id": source_id,
                    "file_path": file_path,
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "indexed_at": datetime.utcnow().isoformat(),

                    # –í—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
                    **{k: v for k, v in meta.items()
                       if k not in ["source_id", "file_path"]}
                }
                for i, chunk in enumerate(chunks)
            ]
        }

        # 6. –í—Å—Ç–∞–≤–ª—è–µ–º —á–µ—Ä–µ–∑ insert_custom_kg
        # LightRAG –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á–µ—Ç entities –∏ relationships
        track_id = await rag.ainsert_custom_kg(custom_kg)

        return {
            "track_id": track_id,
            "source_id": source_id,
            "chunks_created": len(chunks),
            "metadata_applied": True,
            "filename": file.filename
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"Processing error: {str(e)}")


@router.post("/query_with_filter")
async def query_with_filter(request: QueryWithFilterRequest):
    """
    –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º

    Example:
        {
            "query": "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏",
            "metadata_filter": {
                "department": "—Ñ–∏–Ω–∞–Ω—Å—ã",
                "category": "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
                "author": "–ò–≤–∞–Ω–æ–≤"
            },
            "mode": "hybrid",
            "include_metadata": true
        }
    """

    try:
        # 1. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
        all_chunks = await rag.kv_storage.get_all_chunks()

        # 2. –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        filtered_source_ids = set()

        for chunk_id, chunk_data in all_chunks.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤—Å–µ–º —Ñ–∏–ª—å—Ç—Ä–∞–º
            match = all(
                chunk_data.get(key) == value
                for key, value in request.metadata_filter.items()
            )

            if match:
                filtered_source_ids.add(chunk_data.get("source_id"))

        # 3. –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        if not filtered_source_ids:
            return {
                "answer": "–î–æ–∫—É–º–µ–Ω—Ç—ã —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.",
                "filtered_documents": 0,
                "sources": []
            }

        # 4. –ü–æ–∏—Å–∫ –ø–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        result = await rag.aquery(
            request.query,
            param=QueryParam(
                mode=request.mode,
                ids=list(filtered_source_ids),
                top_k=request.top_k
            )
        )

        # 5. –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–≤–µ—Ç
        if request.include_metadata:
            chunks_with_meta = []

            for chunk_id in result.get("source_chunks", []):
                chunk_data = await rag.kv_storage.get(chunk_id)

                chunks_with_meta.append({
                    "content": chunk_data["content"],
                    "metadata": {
                        k: v for k, v in chunk_data.items()
                        if k not in ["content", "embedding"]
                    }
                })

            result["chunks_with_metadata"] = chunks_with_meta

        result["filtered_documents"] = len(filtered_source_ids)

        return result

    except Exception as e:
        raise HTTPException(500, f"Query error: {str(e)}")


@router.get("/metadata/fields")
async def get_metadata_fields():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ

    Returns:
        {
            "fields": ["author", "department", "category", ...],
            "field_values": {
                "department": ["—Ñ–∏–Ω–∞–Ω—Å—ã", "IT", "HR"],
                "category": ["–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"]
            }
        }
    """

    all_chunks = await rag.kv_storage.get_all_chunks()

    fields = set()
    field_values = {}

    for chunk_data in all_chunks.values():
        for key, value in chunk_data.items():
            if key not in ["content", "embedding", "chunk_index", "total_chunks"]:
                fields.add(key)

                # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if key not in field_values:
                    field_values[key] = set()

                if isinstance(value, (str, int, float, bool)):
                    field_values[key].add(str(value))
                elif isinstance(value, list):
                    field_values[key].update(str(v) for v in value)

    return {
        "fields": sorted(list(fields)),
        "field_values": {
            k: sorted(list(v)) for k, v in field_values.items()
        }
    }


@router.get("/metadata/stats")
async def get_metadata_stats():
    """
    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –≤ –±–∞–∑–µ

    Returns:
        {
            "total_documents": 150,
            "total_chunks": 750,
            "metadata_coverage": {
                "author": 0.95,      // 95% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–º–µ—é—Ç –ø–æ–ª–µ author
                "department": 0.80,
                ...
            }
        }
    """

    all_chunks = await rag.kv_storage.get_all_chunks()
    total_chunks = len(all_chunks)

    # –ü–æ–¥—Å—á–µ—Ç source_id (—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
    source_ids = set(
        chunk.get("source_id")
        for chunk in all_chunks.values()
    )
    total_documents = len(source_ids)

    # –ü–æ–¥—Å—á–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    field_counts = {}

    for chunk_data in all_chunks.values():
        for key in chunk_data.keys():
            if key not in ["content", "embedding"]:
                field_counts[key] = field_counts.get(key, 0) + 1

    metadata_coverage = {
        field: count / total_chunks
        for field, count in field_counts.items()
    }

    return {
        "total_documents": total_documents,
        "total_chunks": total_chunks,
        "metadata_coverage": metadata_coverage,
        "avg_chunks_per_document": total_chunks / total_documents if total_documents > 0 else 0
    }
```

### –ö–æ–¥: `lightrag/api/utils/file_parsers.py`

```python
"""
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤
"""

from fastapi import UploadFile, HTTPException
from pathlib import Path
import io

# –ò–º–ø–æ—Ä—Ç—ã –ø–∞—Ä—Å–µ—Ä–æ–≤
try:
    import PyPDF2
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

try:
    from docx import Document
    DOCX_SUPPORT = True
except ImportError:
    DOCX_SUPPORT = False

try:
    import pandas as pd
    CSV_SUPPORT = True
except ImportError:
    CSV_SUPPORT = False


SUPPORTED_EXTENSIONS = {
    '.txt', '.md', '.pdf', '.docx', '.doc',
    '.csv', '.json', '.xml'
}


async def parse_file(file: UploadFile) -> str:
    """
    –ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã LightRAG

    Args:
        file: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª

    Returns:
        str: –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞

    Raises:
        HTTPException: –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    """

    file_ext = Path(file.filename).suffix.lower()

    if file_ext not in SUPPORTED_EXTENSIONS:
        raise HTTPException(
            415,
            f"Unsupported file type: {file_ext}. "
            f"Supported: {', '.join(SUPPORTED_EXTENSIONS)}"
        )

    file_content = await file.read()

    # PDF
    if file_ext == '.pdf':
        if not PDF_SUPPORT:
            raise HTTPException(500, "PDF support not installed. Install PyPDF2")
        return _parse_pdf(file_content)

    # DOCX
    elif file_ext == '.docx':
        if not DOCX_SUPPORT:
            raise HTTPException(500, "DOCX support not installed. Install python-docx")
        return _parse_docx(file_content)

    # CSV
    elif file_ext == '.csv':
        if not CSV_SUPPORT:
            raise HTTPException(500, "CSV support not installed. Install pandas")
        return _parse_csv(file_content)

    # JSON
    elif file_ext == '.json':
        return _parse_json(file_content)

    # XML
    elif file_ext == '.xml':
        return _parse_xml(file_content)

    # Plain text
    elif file_ext in ['.txt', '.md']:
        return _parse_text(file_content)

    else:
        # –ü–æ–ø—ã—Ç–∫–∞ –∫–∞–∫ —Ç–µ–∫—Å—Ç
        return _parse_text(file_content)


def _parse_pdf(content: bytes) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF"""
    pdf_file = io.BytesIO(content)
    reader = PyPDF2.PdfReader(pdf_file)

    text_parts = []
    for page_num, page in enumerate(reader.pages, 1):
        text = page.extract_text()
        text_parts.append(f"--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ---\n{text}\n")

    return "\n".join(text_parts)


def _parse_docx(content: bytes) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ DOCX"""
    doc_file = io.BytesIO(content)
    doc = Document(doc_file)

    text_parts = []

    # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã
    for para in doc.paragraphs:
        if para.text.strip():
            text_parts.append(para.text)

    # –¢–∞–±–ª–∏—Ü—ã
    for table in doc.tables:
        for row in table.rows:
            row_text = " | ".join(cell.text for cell in row.cells)
            text_parts.append(row_text)

    return "\n\n".join(text_parts)


def _parse_csv(content: bytes) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV"""
    try:
        df = pd.read_csv(io.BytesIO(content))
        return df.to_string()
    except Exception as e:
        raise HTTPException(400, f"Failed to parse CSV: {str(e)}")


def _parse_json(content: bytes) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç JSON –≤ —Ç–µ–∫—Å—Ç"""
    import json

    try:
        data = json.loads(content.decode('utf-8'))
        return json.dumps(data, ensure_ascii=False, indent=2)
    except json.JSONDecodeError as e:
        raise HTTPException(400, f"Invalid JSON: {str(e)}")


def _parse_xml(content: bytes) -> str:
    """–ü–∞—Ä—Å–∏—Ç XML"""
    import xml.etree.ElementTree as ET

    try:
        root = ET.fromstring(content.decode('utf-8'))
        return ET.tostring(root, encoding='unicode', method='text')
    except ET.ParseError as e:
        raise HTTPException(400, f"Invalid XML: {str(e)}")


def _parse_text(content: bytes) -> str:
    """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã"""
    for encoding in ['utf-8', 'latin-1', 'cp1251']:
        try:
            return content.decode(encoding)
        except UnicodeDecodeError:
            continue

    raise HTTPException(400, "Unable to decode file with supported encodings")
```

### –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤ `lightrag/api/main.py`

```python
from fastapi import FastAPI
from .routes import documents, documents_metadata

app = FastAPI()

# –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–æ—É—Ç—ã
app.include_router(documents.router)

# –ù–û–í–´–ï —Ä–æ—É—Ç—ã –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
app.include_router(documents_metadata.router)
```

---

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 1. –ó–∞–≥—Ä—É–∑–∫–∞ PDF —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

```bash
curl -X POST "http://localhost:9621/documents/upload_with_metadata" \
  -F "file=@/path/to/quarterly_report_q1_2025.pdf" \
  -F 'metadata={
    "source_id": "report-q1-2025",
    "author": "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á",
    "department": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª",
    "date_created": "2025-01-15",
    "category": "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
    "subcategory": "–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç",
    "tags": ["Q1", "2025", "–≤–∞–∂–Ω—ã–π", "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ"],
    "confidentiality": "internal",
    "approver": "–ü–µ—Ç—Ä–æ–≤ –ü.–ü.",
    "status": "approved"
  }' \
  -F "chunk_size=1200" \
  -F "chunk_overlap=200"
```

**–û—Ç–≤–µ—Ç:**

```json
{
  "track_id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
  "source_id": "report-q1-2025",
  "chunks_created": 12,
  "metadata_applied": true,
  "filename": "quarterly_report_q1_2025.pdf"
}
```

### 2. –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º

```bash
curl -X POST "http://localhost:9621/documents/query_with_filter" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "–∫–∞–∫–æ–≤–∞ –≤—ã—Ä—É—á–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏?",
    "metadata_filter": {
      "department": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª",
      "category": "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
      "status": "approved"
    },
    "mode": "hybrid",
    "include_metadata": true
  }'
```

**–û—Ç–≤–µ—Ç:**

```json
{
  "answer": "–í—ã—Ä—É—á–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏ –≤ Q1 2025 —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 150 –º–ª–Ω —Ä—É–±–ª–µ–π, —á—Ç–æ –Ω–∞ 20% –≤—ã—à–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∫–≤–∞—Ä—Ç–∞–ª–∞...",
  "filtered_documents": 2,
  "chunks_with_metadata": [
    {
      "content": "–í –ø–µ—Ä–≤–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ 2025 –≥–æ–¥–∞ –≤—ã—Ä—É—á–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 150 –º–ª–Ω —Ä—É–±–ª–µ–π...",
      "metadata": {
        "source_id": "report-q1-2025",
        "file_path": "quarterly_report_q1_2025.pdf",
        "chunk_index": 3,
        "total_chunks": 12,
        "author": "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á",
        "department": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª",
        "date_created": "2025-01-15",
        "category": "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
        "tags": ["Q1", "2025", "–≤–∞–∂–Ω—ã–π"],
        "status": "approved"
      }
    }
  ],
  "sources": [
    {
      "source_id": "report-q1-2025",
      "relevance_score": 0.95
    }
  ]
}
```

### 3. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö

```bash
curl "http://localhost:9621/documents/metadata/fields"
```

**–û—Ç–≤–µ—Ç:**

```json
{
  "fields": [
    "author",
    "category",
    "date_created",
    "department",
    "source_id",
    "status",
    "tags"
  ],
  "field_values": {
    "department": ["IT", "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª", "HR"],
    "category": ["—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "–ø–æ–ª–∏—Ç–∏–∫–∏"],
    "status": ["approved", "draft", "review"]
  }
}
```

### 4. Node-RED –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

```javascript
// Node-RED Function node

// –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –Ω–æ–¥—ã
const fileBuffer = msg.payload;

// –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
const metadata = {
    source_id: msg.documentId || `doc-${Date.now()}`,
    author: msg.author || "System",
    department: msg.department || "General",
    date_created: new Date().toISOString().split('T')[0],
    category: msg.category || "general",
    tags: msg.tags || [],
    custom_field: msg.customData
};

// –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ multipart/form-data
const FormData = require('form-data');
const form = new FormData();

form.append('file', fileBuffer, msg.filename);
form.append('metadata', JSON.stringify(metadata));
form.append('chunk_size', 1200);
form.append('chunk_overlap', 200);

msg.payload = form;
msg.headers = form.getHeaders();
msg.url = 'http://lightrag:9621/documents/upload_with_metadata';
msg.method = 'POST';

return msg;
```

---

## –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞

| –í–æ–ø—Ä–æ—Å | –û—Ç–≤–µ—Ç |
|--------|-------|
| **–†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤?** | ‚úÖ –î–∞: PDF, DOCX, TXT, CSV, JSON, XML (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã LightRAG) |
| **–õ—é–±—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ?** | ‚úÖ –î–∞: –ª—é–±—ã–µ –ø–æ–ª—è, –Ω–µ—Ç —Å—Ö–µ–º—ã, –ø–æ–ª–Ω–∞—è –≥–∏–±–∫–æ—Å—Ç—å |
| **–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π chunking?** | ‚úÖ –î–∞: `ChunkedTextSplitter` –∏–∑ LightRAG —Å tokenizer |
| **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ entities?** | ‚úÖ –î–∞: —á–µ—Ä–µ–∑ `insert_custom_kg` ‚Üí LLM –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç |
| **–ü–æ–∏—Å–∫ –ø–æ source_id?** | ‚úÖ –î–∞: –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä `ids` |
| **–ü–æ–∏—Å–∫ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º?** | ‚úÖ –î–∞: —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π endpoint `/query_with_filter` |
| **–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö?** | ‚úÖ –î–∞: `include_metadata=true` |
| **REST API?** | ‚úÖ –î–∞: –ø–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —á–µ—Ä–µ–∑ FastAPI |
| **–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å?** | ‚úÖ –î–∞: —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ endpoints –Ω–µ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã |
| **–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞?** | üïê 0.5-1 –¥–µ–Ω—å (3 —Ñ–∞–π–ª–∞: —Ä–æ—É—Ç, –ø–∞—Ä—Å–µ—Ä—ã, —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è) |

---

## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–∏—Å–æ–º

1. ‚úÖ **–ß–∏—Å—Ç–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ** - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è, –Ω–µ –≤ —Ç–µ–∫—Å—Ç–µ
2. ‚úÖ **–ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø** - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ª—é–±—ã–º –ø–æ–ª—è–º –±–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞
3. ‚úÖ **–í—Å—Ç—Ä–æ–µ–Ω–æ –≤ LightRAG** - –Ω–µ –Ω—É–∂–µ–Ω –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å
4. ‚úÖ **–ú–µ–Ω—å—à–µ –∫–æ–¥–∞** - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã LightRAG
5. ‚úÖ **–ü—Ä–æ—â–µ –ø–æ–¥–¥–µ—Ä–∂–∫–∞** - –æ–¥–∏–Ω —Å–µ—Ä–≤–∏—Å –≤–º–µ—Å—Ç–æ –¥–≤—É—Ö
6. ‚úÖ **–ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å PR** - –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –≤—Å–µ–≥–æ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ LightRAG

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. –§–æ—Ä–∫–Ω—É—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π LightRAG
2. –°–æ–∑–¥–∞—Ç—å –≤–µ—Ç–∫—É `feature/metadata-endpoints`
3. –î–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª—ã:
   - `lightrag/api/routes/documents_metadata.py`
   - `lightrag/api/utils/file_parsers.py`
4. –û–±–Ω–æ–≤–∏—Ç—å `lightrag/api/main.py`
5. –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã
6. –°–æ–∑–¥–∞—Ç—å Pull Request –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π

–ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º!
